{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8BUKn/RyIxpBveWPC7wEm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#BIBLIOTECAS\n",
        "!pip install pdfplumber\n",
        "!pip install supabase\n",
        "\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from supabase import create_client, Client\n",
        "import pdfplumber\n",
        "from typing import Dict, Any, List\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ejcAUHiP_yps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "biblioteca_produtos = {\n",
        "    \"CALDO KNORR\": \"Caldo Knorr\",\n",
        "    \"COSTELA TRAS PART FC\": \"Costela Traseira Bovina Friboi\",\n",
        "    \"PATINHO BOV FC kg\": \"Patinho Bovino Friboi Kg\",\n",
        "    \"CONTRA FILE BIFE BJ\": \"Contra Filé Bife Bovino\",\n",
        "    \"PICANHA FRIBOI BJ\": \"Picanha Friboi\",\n",
        "    \"COXAO MOLE BOV FC kg\": \"Coxão Mole Bovino Friboi Kg\",\n",
        "    \"LI SU AURORA 600G FI\": \"Linguiça Suína Aurora 600g Fina\",\n",
        "    \"LI CAL D SEARA 400G\": \"Linguiça Calabresa Defumada Seara 400g\",\n",
        "    \"P ALHO ZINHO 300G TR\": \"Pão de Alho Zinho 300g Tradicional\",\n",
        "    \"IOG NESTLE 1250G VIT\": \"Iogurte Nestlé 1250g Vitamina\",\n",
        "    \"IOG NESTLE 170G C/LA\": \"Iogurte Nestlé 170g com Lactobacilos\",\n",
        "    \"IOG NESTLE 170G MEL\": \"Iogurte Nestlé 170g Mel\",\n",
        "    \"RQ CREM VIGOR C 200G\": \"Requeijão Cremoso Vigor Copo 200g\",\n",
        "    \"PARM VIGOR FAT 200G\": \"Queijo Parmesão Vigor Fatia 200g\",\n",
        "    \"BACON SADIA PD\": \"Bacon Sadia em Pedaços\",\n",
        "    \"FILE PEITO SEARA 1kg\": \"Filé de Peito de Frango Seara 1kg\",\n",
        "    \"PEI PERU DEF PERD FT\": \"Peito de Peru Defumado Perdigão Fatiado\",\n",
        "    \"MUSS BETANIA FT\": \"Queijo Mussarela Betânia Fatiado\",\n",
        "    \"COCO VERDE\": \"Coco Verde\",\n",
        "    \"ALFACE CRESPA\": \"Alface Crespa\",\n",
        "    \"COUVE MANTEIGA\": \"Couve Manteiga\",\n",
        "    \"COENTRO UN\": \"Coentro Unidade\",\n",
        "    \"CEBOLINHA UN\": \"Cebolinha Unidade\",\n",
        "    \"BANANA PRATA kg\": \"Banana Prata Kg\",\n",
        "    \"BANANA NANICA kg\": \"Banana Nanica Kg\",\n",
        "    \"CENOURA kg\": \"Cenoura Kg\",\n",
        "    \"TANGER MURC kg\": \"Tangerina Murcote Kg\",\n",
        "    \"LIMAO THAITI kg\": \"Limão Taiti Kg\",\n",
        "    \"BAC SEARA TABL kg\": \"Bacon Seara Tablado Kg\",\n",
        "    \"MACA NAC GALA kg\": \"Maçã Nacional Gala Kg\",\n",
        "    \"PIM DE CHEIRO kg\": \"Pimentinha de Cheiro Kg\",\n",
        "    \"MELAO AMARELO kg\": \"Melão Amarelo Kg\",\n",
        "    \"MORANGO SERRA SUL PR\": \"Morango Serra Sul Bandeja\",\n",
        "    \"CERVEJA CORONA 330ML\": \"Cerveja Corona 330ml\",\n",
        "    \"CERV EISENBAHN 350ML\": \"Cerveja Eisenbahn 350ml\",\n",
        "    \"CHA LEAO 25G 10SQ AB\": \"Chá Leão 25g 10 Sachês Abacaxi\",\n",
        "    \"CHA LEAO ICE TEA\": \"Chá Leão Ice Tea\",\n",
        "    \"CH V LEAO 25G GEN/LI\": \"Chá Verde Leão 25g Gengibre e Limão\",\n",
        "    \"GUARANA ANT ZERO 2LT\": \"Guaraná Antarctica Zero 2L\",\n",
        "    \"REF S GERAL 2L CAJU\": \"Refrigerante São Geraldo 2L Caju\",\n",
        "    \"REF GUAR ANTARCTICA\": \"Refrigerante Guaraná Antarctica\",\n",
        "    \"SUCO OQ UVA 100 SUCO\": \"Suco OQ Uva 100% Suco\",\n",
        "    \"LTE INT VIT BETAN 1L\": \"Leite Integral Betânia 1L\",\n",
        "    \"BEB ITALAC 200ML VIT\": \"Bebida Láctea Italac 200ml Vitamina\",\n",
        "    \"BEB ITALAKINHO 200ML\": \"Bebida Italakinho 200ml\",\n",
        "    \"L COND ITAL Z L 395G\": \"Leite Condensado Italac Zero Lactose 395g\",\n",
        "    \"CR LEI BETANIA 200G\": \"Creme de Leite Betânia 200g\",\n",
        "    \"ATUM R O GCOSTA 130G\": \"Atum Ralado ao Óleo Gomes da Costa 130g\",\n",
        "    \"M PIM CHIP MEND 80ML\": \"Molho Pimenta Chipotle Mendes 80ml\",\n",
        "    \"NEWAFER 100G MOR\": \"Wafer Newafer 100g Morango\",\n",
        "    \"BIS CHOC BAUD 80G LE\": \"Biscoito Bauducco Chocolate ao Leite 80g\",\n",
        "    \"CLUB INT 288G\": \"Biscoito Club Social Integral 288g\",\n",
        "    \"AMAN MARILAN 280G CH\": \"Amanteigado Marilan 280g Chocolate\",\n",
        "    \"ACHOC PO NESCAU 900G\": \"Achocolatado em Pó Nescau 900g\",\n",
        "    \"ACHOC PO NESCAU 730G\": \"Achocolatado em Pó Nescau 730g\",\n",
        "    \"WAF ESCUT 80G BAUN\": \"Wafer Escureto 80g Baunilha\",\n",
        "    \"WAF PIRAQ 100G LIMAO\": \"Wafer Piraquê 100g Limão\",\n",
        "    \"WAF AMORI 80G MOR\": \"Wafer Amori 80g Morango\",\n",
        "    \"WAF AMORI 80G CHOC\": \"Wafer Amori 80g Chocolate\",\n",
        "    \"WAF AMORI 80G M LIM\": \"Wafer Amori 80g Mousse Limão\",\n",
        "    \"WAF MAX 87G AO LEITE\": \"Wafer Max 87g Ao Leite\",\n",
        "    \"SALG FAND 160G CHURR\": \"Salgadinho Fandangos 160g Churrasco\",\n",
        "    \"SALG KRO 70G REQ\": \"Salgadinho Kro 70g Requeijão\",\n",
        "    \"FEIJ CAR KICALDO 1kg\": \"Feijão Carioca Kicaldo 1kg\",\n",
        "    \"MAC DB 500G PENNE\": \"Macarrão Dona Benta 500g Penne\",\n",
        "    \"ARR NAMORADO 1kg BCO\": \"Arroz Namorado 1kg Branco\",\n",
        "    \"TOALHA HIP SNOB C/3R\": \"Toalha Higiênica Snob com 3 Rolos\",\n",
        "    \"SALG CHEETO 143G LUA\": \"Salgadinho Cheetos 143g Lua\",\n",
        "    \"DORITOS NACHO 300G\": \"Doritos Nacho 300g\",\n",
        "    \"SALG DORIT 32G SWEET\": \"Doritos Sweet Chili 32g\",\n",
        "    \"MASSA RAP10 297G TRA\": \"Massa Rap10 297g Tradicional\",\n",
        "    \"TOALHA HUG T P C/120\": \"Lenço Umedecido Huggies Toque de Proteção c/120\",\n",
        "    \"MAC N SABOR 74G G CA\": \"Macarrão Instantâneo Nissin Sabor Galinha Caipira 74g\",\n",
        "    \"MAC N SABOR 74G GAL\": \"Macarrão Instantâneo Nissin Sabor Galinha 74g\",\n",
        "    \"MAC N S 74G CAR PIC\": \"Macarrão Instantâneo Nissin Sabor Carne Picante 74g\",\n",
        "    \"SBT NIVEA 80G MANT\": \"Sabonete Nivea 80g Manteiga\",\n",
        "    \"SBT NIVEA 85G AVEIA\": \"Sabonete Nivea 85g Aveia\",\n",
        "    \"SBT NIVEA 85G ERV DC\": \"Sabonete Nivea 85g Erva Doce\",\n",
        "    \"SBT NIV 125G AO LEIT\": \"Sabonete Nivea 125g Ao Leite\",\n",
        "    \"OVO BC AVINE 30UN\": \"Ovos Brancos Avine 30 Unidades\",\n",
        "    \"MULTIUSO MEMBER S 1\": \"Multiuso Member's Mark 1L\",\n",
        "    \"YPE LIQ 6X500ML DETE\": \"Detergente Ypê Líquido 6x500ml\",\n",
        "    \"MR MUSCULO COZINHA P\": \"Mr. Músculo Limpeza Cozinha\",\n",
        "    \"SACOLA SAMS GRD BRAN\": \"Sacola Sam's Club Grande Branca\",\n",
        "    \"BATATA PRE FRITA FIN\": \"Batata Pré-Frita Fina\",\n",
        "    \"PAO ALHO BAGUETE TRA\": \"Pão de Alho Baguete Tradicional\",\n",
        "    \"FANDANGOS PRESUNTO 2\": \"Fandangos Sabor Presunto 200g\",\n",
        "    \"PH VELUD FD L24 P22\": \"Papel Higiênico Veludíssimo Fardo 24 Rolos\",\n",
        "    \"AC REF UNIAO 1kg\": \"Açúcar Refinado União 1kg\",\n",
        "\t\"FILE PEIT REGINA 700\": \"Filé de Peito de Frango Regina 700g\",\n",
        "    \"PEITO PERU SADIA FAT\": \"Peito de Peru Sadia Fatiado\",\n",
        "    \"FILE PEITO SADIA\": \"Filé de Peito de Frango Sadia\",\n",
        "    \"COX.ASA FGO SADIA\": \"Coxinha da Asa de Frango Sadia\",\n",
        "    \"FILE PEITO SEARA 1kg\": \"Filé de Peito de Frango Seara 1kg\",\n",
        "    \"FGO ASSADO SL 700 G\": \"Frango Assado SL 700g\",\n",
        "    \"NUGGETS FRANGO ORIGI\": \"Nuggets de Frango Original\",\n",
        "    \"COX.FGO SEARA IQF\": \"Coxa de Frango Seara IQF (Congelado Individualmente)\",\n",
        "    \"RF.SELETA SEARA\": \"Refogado Seleta de Frango Seara\",\n",
        "    \"FILE PEITO S/OSSO\": \"Filé de Peito de Frango Sem Osso\",\n",
        "    \"CHICKEN SUPREM SEARA\": \"Chicken Supreme Seara (Empanado de Frango)\",\n",
        "    \"PEITO FRANGO C/OSSO\": \"Peito de Frango com Osso\",\n",
        "    \"CORACAO SADIA 1KG\": \"Coração de Frango Sadia 1kg\",\n",
        "    \"PEITO FGO RESF 700G\": \"Peito de Frango Resfriado 700g\",\n",
        "    \"COXA FGO S/P 700G\": \"Coxa de Frango Sem Pele 700g\",\n",
        "    \"BIFE PATINHO 1953 KG\": \"Bife de Patinho 1953 Kg\",\n",
        "    \"BIFE CX MOLE 1953 KG\": \"Bife de Coxão Mole 1953 Kg\",\n",
        "    \"COXAO DURO BOV FC kg\": \"Coxão Duro Bovino Friboi Kg\",\n",
        "    \"COXAO MOLE BOV FC kg\": \"Coxão Mole Bovino Friboi Kg\",\n",
        "    \"LAGARTO BOV 1953 RE\": \"Lagarto Bovino 1953 Reserva\",\n",
        "    \"STROG PATINHO 1953 K\": \"Strogonoff de Patinho 1953 Kg\",\n",
        "    \"BOV.COXAO MOLE RESER\": \"Bovino Coxão Mole Reserva\",\n",
        "    \"BOV.PATINHO MOIDO RE\": \"Bovino Patinho Moído Reserva\",\n",
        "    \"BOV.MIOLO ALCATRA RE\": \"Bovino Miolo de Alcatra Reserva\",\n",
        "    \"BOV.PATINHO S/OSSO R\": \"Bovino Patinho Sem Osso Reserva\",\n",
        "    \"BOV.MAMINHA RESERVA\": \"Bovino Maminha Reserva\",\n",
        "    \"BOV.BIFE MIO.ALC.RES\": \"Bovino Bife de Miolo de Alcatra Reserva\",\n",
        "    \"CONTRA FILE BOV RESF\": \"Contra Filé Bovino Resfriado\",\n",
        "    \"STROGONOFF CX MOLE\": \"Strogonoff de Coxão Mole\",\n",
        "    \"SCALOPE FILE MINGKG\": \"Scolope de Filé Mignon Kg\",\n",
        "    \"BIFE MILANESA PATINH\": \"Bife à Milanesa de Patinho\",\n",
        "    \"LI CAL D PERDIG 400G\": \"Linguiça Calabresa Defumada Perdigão 400g\",\n",
        "    \"LING.T.CAL.PERDIGAO\": \"Linguiça Tipo Calabresa Perdigão\",\n",
        "    \"BACON PDC SEARA\": \"Bacon em Pedaço Seara\",\n",
        "    \"LING.T.CALAB.SADIA\": \"Linguiça Tipo Calabresa Sadia\",\n",
        "    \"BACON SEARA DEF.\": \"Bacon Seara Defumado\",\n",
        "    \"JERKED SEARA SUINO\": \"Jerky Suíno Seara\",\n",
        "    \"LING.CALAB.PERDIGAO\": \"Linguiça Calabresa Perdigão\",\n",
        "    \"SALSICHA AURORA CONG\": \"Salsicha Aurora Congelada\",\n",
        "    \"JERKEED B.DIANTEIRO\": \"Jerkeed Bovino Dianteiro\",\n",
        "    \"PRES SUIN SOLTISS200\": \"Presunto Suíno Soltíssimo 200g\",\n",
        "    \"MORT OURO PERD FAT\": \"Mortadela Ouro Perdigão Fatiada\",\n",
        "    \"MORT.SEARA DEF.FAT\": \"Mortadela Seara Defumada Fatiada\",\n",
        "    \"QJ MUSS TIROLEZ FAT\": \"Queijo Mussarela Tirolez Fatiado\",\n",
        "    \"REQ CATUP 420G TRAD\": \"Requeijão Catupiry Tradicional 420g\",\n",
        "    \"QJO MUSS.BETANIA FAT\": \"Queijo Mussarela Betânia Fatiado\",\n",
        "    \"QJO PARMESAO TIROLEZ\": \"Queijo Parmesão Tirolez\",\n",
        "    \"CHEDDAR POLENGHI\": \"Queijo Cheddar Polenguhi\",\n",
        "    \"QJ MUSS PRESIDENT 15\": \"Queijo Mussarela President 150g\",\n",
        "    \"MUSSARELA NATVILLE\": \"Mussarela Natville\",\n",
        "    \"PEITO PERU DEF.L.FAT\": \"Peito de Peru Defumado Light Fatiado\",\n",
        "    \"QJO.CHEDDAR PROC.\": \"Queijo Cheddar Processado\",\n",
        "    \"QJ PAR RAL PRES 50G\": \"Queijo Parmesão Ralado President 50g\",\n",
        "    \"QJ SAND CHEDAR FAT\": \"Queijo para Sanduíche Sabor Cheddar Fatiado\",\n",
        "    \"QJ MUC IFAT KG\": \"Queijo Muçarela Fatiado Kg\",\n",
        "    \"REQ.CATUPIRY TRAD.\": \"Requeijão Catupiry Tradicional\",\n",
        "    \"CREAM C.PHILADELPHIA\": \"Cream Cheese Philadelphia\",\n",
        "    \"QJ PARM PED NOAL\": \"Queijo Parmesão em Pedaço Noal\",\n",
        "    \"BISC BONO 90G MOR\": \"Biscoito Bono 90g Morango\",\n",
        "    \"WAF AMORI 80G CHOC\": \"Biscoito Wafer Amori 80g Chocolate\",\n",
        "    \"BISC BONO 90G CHOC\": \"Biscoito Bono 90g Chocolate\",\n",
        "    \"CLUB INT 144G INT\": \"Biscoito Club Social 144g Integral\",\n",
        "    \"WAF AMORI 80G MOR\": \"Biscoito Wafer Amori 80g Morango\",\n",
        "    \"WAF AMORI 80G M LIM\": \"Biscoito Wafer Amori 80g Mousse de Limão\",\n",
        "    \"SUCRILHOS 700G ORIG\": \"Sucrilhos Kellogg's 700g Original\",\n",
        "    \"SALG CHEET 270G ONDA\": \"Salgadinho Cheetos 270g Onda\",\n",
        "    \"SALG FAND 160G QJO\": \"Salgadinho Fandangos 160g Queijo\",\n",
        "    \"BISC.BAUDUCCO WAFER\": \"Biscoito Bauducco Wafer\",\n",
        "    \"BATATA PALHA YOKI\": \"Batata Palha Yoki\",\n",
        "    \"BISC.CLUB SOCIAL\": \"Biscoito Club Social\",\n",
        "    \"AMAN MARILAN 280G CH\": \"Biscoito Amanteigado Marilan 280g Chocolate\",\n",
        "    \"WAF PIRAQ 100G LIMAO\": \"Biscoito Wafer Piraquê 100g Limão\",\n",
        "    \"PALHA ELMA 215G TRAD\": \"Batata Palha Elma Chips 215g Tradicional\",\n",
        "    \"SALG.ELMA CHIPS\": \"Salgadinho Elma Chips\",\n",
        "    \"BISC.RICHESTER\": \"Biscoito Richester\",\n",
        "    \"AMENDOIM S.HELENA CR\": \"Amendoim Santa Helena Crocante\",\n",
        "    \"ACHOC PO NESCAU 730G\": \"Achocolatado em Pó Nescau 730g\",\n",
        "    \"CHA LEAO 25G 10SQ AB\": \"Chá Leão 25g com 10 sachês Abacaxi\",\n",
        "    \"CHA LEAO 16G MATTE/G\": \"Chá Leão 16g Matte e Gengibre\",\n",
        "    \"ACHOC.NESCAU MENOS\": \"Achocolatado Nescau Menos Açúcar\",\n",
        "    \"NESQUIK MORANGO LATA\": \"Achocolatado Nesquik Morango Lata\",\n",
        "    \"CAFE S CL SOL VD100G\": \"Café Santa Clara Solúvel Vidro 100g\",\n",
        "    \"CHA LEAO COLD BREW\": \"Chá Leão Cold Brew (Infusão a Frio)\",\n",
        "    \"NESCAFE ORIG.SACHET\": \"Nescafé Original Sachet\",\n",
        "    \"CAFE 3CORACOES CAPPU\": \"Cappuccino 3 Corações\",\n",
        "    \"CHA MATTE LEAO\": \"Chá Matte Leão\",\n",
        "    \"Chá Verde Leão 25g Gengibre E Limão\": \"Chá Verde Leão 25g Gengibre e Limão\",\n",
        "    \"D.PACOCA S.HELENA\": \"Doce de Paçoca Santa Helena\",\n",
        "    \"DOCE LEITE OLIVEIRA\": \"Doce de Leite Oliveira\",\n",
        "    \"BALA ARCOR SETE BELO\": \"Bala Arcor Sete Belo\",\n",
        "    \"L COND ITALAC 395G\": \"Leite Condensado Italac 395g\",\n",
        "    \"L.COND.PIRACA.ZERO L\": \"Leite Condensado Piracanjuba Zero Lactose\",\n",
        "    \"GOIABANA DANTAS 250G\": \"Goiabada Dantas 250g\",\n",
        "    \"GELATINA DR.OETKER\": \"Gelatina Dr. Oetker\",\n",
        "    \"GELEIA HERO\": \"Geleia Hero\",\n",
        "    \"COBERT.CHOC.HARALD\": \"Cobertura de Chocolate Harald\",\n",
        "    \"ATUM G.COSTA RAL.OLE\": \"Atum Gomes da Costa Ralado em Óleo\",\n",
        "    \"ATUM R O GCOSTA 130G\": \"Atum Ralado em Óleo Gomes da Costa 130g\",\n",
        "    \"AZ.VDE LA VIOL FAT.\": \"Azeitona Verde La Violetera Fatiada\",\n",
        "    \"MILHO VERDE QUERO\": \"Milho Verde Quero Lata\",\n",
        "    \"ERVILHA QUERO LATA\": \"Ervilha Quero Lata\",\n",
        "    \"PEPINO HEMMER ROD.\": \"Pepino em Conserva Hemmer em Rodelas\",\n",
        "    \"ARROZ NAMORADO AG.T1\": \"Arroz Namorado Agulhinha Tipo 1\",\n",
        "    \"FEIJAO CAMIL\": \"Feijão Carioca Camil\",\n",
        "    \"FEIJAO CAMIL PTO\": \"Feijão Preto Camil\",\n",
        "    \"FAR.TRIGO D.BENTA\": \"Farinha de Trigo Dona Benta\",\n",
        "    \"AMIDO MILHO MAIZENA\": \"Amido de Milho Maizena\",\n",
        "    \"FERM PO ROYAL 100G\": \"Fermento em Pó Royal 100g\",\n",
        "    \"ARROZ AG CAMIL T1 1K\": \"Arroz Agulhinha Camil Tipo 1 1kg\",\n",
        "    \"AVEIA QUAKER\": \"Aveia Quaker\",\n",
        "    \"MAC NISSIN BACON 85G\": \"Macarrão Instantâneo Nissin Lámen Sabor Bacon 85g\",\n",
        "    \"MAC.D.BENTA SEMOLA\": \"Macarrão de Sêmola Dona Benta\",\n",
        "    \"MASSA PAST MASSA LV\": \"Massa para Pastel Massa Leve\",\n",
        "    \"MAC DB 500G NINHO\": \"Macarrão Dona Benta 500g Ninho\",\n",
        "    \"MAC DB 500G SPAG\": \"Macarrão Dona Benta 500g Spaghetti\",\n",
        "    \"MAC.NISSIN LAMEN\": \"Macarrão Instantâneo Nissin Lámen\",\n",
        "    \"MAC ADR GRANO ESPAG\": \"Macarrão Adria Grano Duro Spaghetti\",\n",
        "    \"MASSA RAP10 297G TRA\": \"Massa Rápida Rap10 297g Tradicional\",\n",
        "    \"FAGOTTINI MASSA LEVE\": \"Fagottini Massa Leve\",\n",
        "    \"TEMP.SABOR AMI\": \"Tempero Sabor a Mi\",\n",
        "    \"CATCHUP HEINZ PET\": \"Catchup Heinz Embalagem PET\",\n",
        "    \"MAION.HELLMANNS\": \"Maionese Hellmann's\",\n",
        "    \"MOLHO TOM.POMAROLA\": \"Molho de Tomate Pomarola\",\n",
        "    \"QA KETCHUP TRAD 400G\": \"Ketchup Quero Tradicional 400g\",\n",
        "    \"OREGANO KITANO\": \"Orégano Kitano\",\n",
        "    \"ACUC.REF.UNIAO\": \"Açúcar Refinado União\",\n",
        "    \"VINAGRE MARATA MACA\": \"Vinagre de Maçã Maratá\",\n",
        "    \"SAL LEBRE REFINADO\": \"Sal Refinado Lebre\",\n",
        "    \"OLEO SOJA LIZA\": \"Óleo de Soja Liza\",\n",
        "    \"UVA THOMPSON BJ 500G\": \"Uva Thompson Bandeja 500g\",\n",
        "    \"MACA RED IMPORT kg\": \"Maçã Red Importada Kg\",\n",
        "    \"BANANA PRATA kg\": \"Banana Prata Kg\",\n",
        "    \"ABACAXI UN\": \"Abacaxi Unidade\",\n",
        "    \"MELANCIA BABY UN\": \"Melancia Baby Unidade\",\n",
        "    \"TOM SWEET GRAPE 180G\": \"Tomate Sweet Grape 180g\",\n",
        "    \"MORANGO NOSSA FRUTA\": \"Morango Nossa Fruta (Bandeja)\",\n",
        "    \"HF.TOMATE GRAPE\": \"Hortifrúti Tomate Grape\",\n",
        "    \"MANGA PALMER NAC KG\": \"Manga Palmer Nacional Kg\",\n",
        "    \"BATATA MONALI LAV kg\": \"Batata Monalisa Lavada Kg\",\n",
        "    \"ALFACE ROXO UNID\": \"Alface Roxo Unidade\",\n",
        "    \"CEBOLA GRANEL KG\": \"Cebola a Granel Kg\",\n",
        "    \"CENOURA KG\": \"Cenoura Kg\",\n",
        "    \"PIMENTAO VERDE kg\": \"Pimentão Verde Kg\",\n",
        "    \"ALFACE CRESPA\": \"Alface Crespa\",\n",
        "    \"COUVE MANTEIGA\": \"Couve Manteiga\",\n",
        "    \"ALHO A GRANEL kg\": \"Alho a Granel Kg\",\n",
        "    \"COENTRO HIDROFOLHAS\": \"Coentro Hidropônico Hidrofolhas\",\n",
        "    \"IOG NESTLE 170G C/LA\": \"Iogurte Nestlé 170g com Lactobacilos\",\n",
        "    \"LTE INT VIT BETAN 1L\": \"Leite Integral Vitaminado Betânia 1L\",\n",
        "    \"LEITE PARMALAT INTEG\": \"Leite Integral Parmalat\",\n",
        "    \"CR.LEITE ITALAC\": \"Creme de Leite Italac\",\n",
        "    \"MARG DORIANA 500G CS\": \"Margarina Doriana 500g com Sal\",\n",
        "    \"MARG BECEL 500G C/SA\": \"Margarina Becel 500g com Sal\",\n",
        "    \"RQ CREM BETANIA 200G\": \"Requeijão Cremoso Betânia 200g\",\n",
        "    \"OVO BCO GRANDE C/30\": \"Ovo Branco Grande com 30 unidades\",\n",
        "    \"LEITE FERM.YAKULT\": \"Leite Fermentado Yakult\",\n",
        "    \"LEITE PO NINHO INT.\": \"Leite em Pó Ninho Integral\",\n",
        "    \"CR LEITE NESTLE 200G\": \"Creme de Leite Nestlé 200g\",\n",
        "    \"REF TANG 18G GUARANA\": \"Refresco em Pó Tang 18g Guaraná\",\n",
        "    \"REF SPRITE 2LT ORIG\": \"Refrigerante Sprite 2L Original\",\n",
        "    \"COCA COLA S/AC 2LT\": \"Refrigerante Coca-Cola Sem Açúcar 2L\",\n",
        "    \"REFRIG S GERALDO 2L\": \"Refrigerante São Geraldo 2L\",\n",
        "    \"REF.KUAT/COCA\": \"Refrigerante Kuat ou Coca-Cola\",\n",
        "    \"GUARANA ANT ZERO 2LT\": \"Refrigerante Guaraná Antarctica Zero 2L\",\n",
        "    \"REF.ANTARC.GUAR.ZERO\": \"Refrigerante Antarctica Guaraná Zero\",\n",
        "    \"VINHO Q.SUAVE BRANCO\": \"Vinho de Mesa Suave Branco\",\n",
        "    \"CER CACILDIS 350ML\": \"Cerveja Cacildis 350ml\",\n",
        "    \"CERV BR EISEB SL 350\": \"Cerveja Eisenbahn Pilsen 350ml\",\n",
        "    \"PAO F BAUD TRAD 390G\": \"Pão de Forma Bauducco Tradicional 390g\",\n",
        "    \"BISNAG PLUSVIT 300G\": \"Bisnaguinha Plus Vita 300g\",\n",
        "    \"PAO CARIOCA SL KG\": \"Pão Carioca SL Kg\",\n",
        "    \"PAO SOVADO SL KG\": \"Pão Sovado SL Kg\",\n",
        "    \"BISNAGUITO PLUS VITA\": \"Bisnaguito Plus Vita\",\n",
        "    \"PAO FORMA BAUDUCCO\": \"Pão de Forma Bauducco\",\n",
        "    \"PULLMAN RAP 10\": \"Massa Rápida Rap10 Pullman\",\n",
        "    \"LXA DE PE PLAST G I\": \"Lixa de Pé Plástica Grande\",\n",
        "    \"PALITO P/CUT C/20\": \"Palito para Cutícula com 20 unidades\",\n",
        "    \"TOALHA UMED.PIQUITUC\": \"Toalha Umedecida Piquitucho\",\n",
        "    \"ESC.D.COLGATE CLEAN\": \"Escova de Dente Colgate Clean\",\n",
        "    \"CR.D.COLGATE T.12\": \"Creme Dental Colgate Total 12\",\n",
        "    \"SAB.PHEBO LIMAO\": \"Sabonete Phebo Limão\",\n",
        "    \"COND.PANTENE\": \"Condicionador Pantene\",\n",
        "    \"SHAMP.HEAD SHOULDERS\": \"Shampoo Head & Shoulders\",\n",
        "    \"ESMALTE RISQUE\": \"Esmalte Risqué\",\n",
        "    \"PANO ESF SCOTCH 2UN\": \"Pano Esponja Scotch-Brite com 2 unidades\",\n",
        "    \"ESP SCOTCH M USO C/4\": \"Esponja Scotch-Brite Multiuso com 4 unidades\",\n",
        "    \"LIMP VEJA 2L LIRIO\": \"Limpador Multiuso Veja 2L Lírio\",\n",
        "    \"LIMP.VEJA LIMP.PESAD\": \"Limpador Veja Limpeza Pesada\",\n",
        "    \"SAC INST.BIO 60X70\": \"Saco para Lixo Institucional Biodegradável 60x70cm\",\n",
        "    \"DET LIMPOL 500ML NEU\": \"Detergente Limpol 500ml Neutro\",\n",
        "    \"ESP BOMBRIL C/8\": \"Esponja de Aço Bombril com 8 unidades\",\n",
        "    \"SACO LIXO GOL 30L\": \"Saco para Lixo Gol 30L\",\n",
        "    \"TOALHA PAPEL SNOB\": \"Toalha de Papel Snob\",\n",
        "    \"FILME PVC GLOBOPACK\": \"Filme de PVC Globopack\",\n",
        "}\n"
      ],
      "metadata": {
        "id": "ffwy87Ilpcyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 1. CONFIGURAÇÃO DO SUPABASE\n",
        "# ==============================================================================\n",
        "supabase_url = \"https://ctjfviyfuwkzuwnoqvhi.supabase.co\"\n",
        "supabase_key = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImN0amZ2aXlmdXdrenV3bm9xdmhpIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTU3MjI2NTcsImV4cCI6MjA3MTI5ODY1N30.FG8B07mYdJLT2Y8-vxn-iV6-biO093nmUEjP90Yg0V8\"\n",
        "\n",
        "try:\n",
        "    supabase: Client = create_client(supabase_url, supabase_key)\n",
        "    print(\"Conexão com Supabase estabelecida com sucesso.\")\n",
        "except Exception as e:\n",
        "    print(f\"Erro ao conectar com Supabase: {e}\")\n",
        "    exit()\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. FUNÇÕES GERAIS\n",
        "# ==============================================================================\n",
        "def extrair_texto_do_pdf(caminho_pdf):\n",
        "    texto_completo = \"\"\n",
        "    try:\n",
        "        with pdfplumber.open(caminho_pdf) as pdf:\n",
        "            for pagina in pdf.pages:\n",
        "                texto_extraido_pagina = pagina.extract_text(x_tolerance=1, y_tolerance=3)\n",
        "                if texto_extraido_pagina:\n",
        "                    texto_completo += texto_extraido_pagina + \"\\n\"\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao ler o PDF '{caminho_pdf}': {e}\")\n",
        "        return None\n",
        "    return texto_completo\n",
        "\n",
        "def normalizar_valor(valor_str):\n",
        "    if isinstance(valor_str, str):\n",
        "        return float(valor_str.replace('.', '').replace(',', '.'))\n",
        "    return float(valor_str)\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. FUNÇÕES DE ANÁLISE ESPECÍFICAS\n",
        "# ==============================================================================\n",
        "\n",
        "# ---------------------------------------------------- ASSAI ----------------------------------------------------\n",
        "def analisar_cupom_assai(texto_bruto):\n",
        "    padrao_item = re.compile(\n",
        "        r'(\\d{3})\\s+(\\d+)\\s+(.+?)\\s*([\\d\\.,]+)\\s+(Kg|Un|PC|La|Fr|Am|Gf|Pt)\\s+X\\s+([\\d\\.,]+)\\s+([\\d\\.,]+)'\n",
        "    )\n",
        "    padrao_data_hora = re.compile(r'(\\d{2}/\\d{2}/\\d{4})\\s+(\\d{2}:\\d{2}:\\d{2})')\n",
        "\n",
        "    dados_cupom = { \"estabelecimento\": {}, \"cupom\": {}, \"itens\": [] }\n",
        "\n",
        "    linhas_brutas = texto_bruto.split('\\n')\n",
        "    if linhas_brutas:\n",
        "        dados_cupom[\"estabelecimento\"][\"nome\"] = \"ASSAI\"\n",
        "\n",
        "    match_data_hora = padrao_data_hora.search(texto_bruto)\n",
        "    if match_data_hora:\n",
        "        dados_cupom[\"cupom\"][\"data_hora\"] = f\"{match_data_hora.group(1)} {match_data_hora.group(2)}\"\n",
        "\n",
        "    for match in padrao_item.finditer(texto_bruto):\n",
        "        dados_cupom[\"itens\"].append({\n",
        "            \"codigo\": match.group(2),\n",
        "            \"descricao\": re.sub(r'\\s+', ' ', match.group(3).replace('\\n', ' ')).strip(),\n",
        "            \"quantidade\": normalizar_valor(match.group(4)),\n",
        "            \"valor_unitario\": normalizar_valor(match.group(6)),\n",
        "            \"valor_total_item\": normalizar_valor(match.group(7))\n",
        "        })\n",
        "    return dados_cupom\n",
        "\n",
        "# ---------------------------------------------------- ATACADAO / MAXXI ----------------------------------------------------\n",
        "def normalizar_valor(valor_str: str) -> float:\n",
        "    \"\"\"Converte uma string de valor monetário para float.\"\"\"\n",
        "    if isinstance(valor_str, (int, float)):\n",
        "        return float(valor_str)\n",
        "    return float(valor_str.replace('.', '').replace(',', '.'))\n",
        "\n",
        "def analisar_cupom_atacadao(texto_bruto: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Analisa o texto bruto de um cupom fiscal do Atacadão/Maxxi e extrai os dados.\n",
        "    \"\"\"\n",
        "    # Regex para capturar a data e hora do cupom\n",
        "    padrao_data_hora = re.compile(r\"(\\d{2}/\\d{2}/\\d{4}\\s+\\d{2}:\\d{2}:\\d{2})\")\n",
        "\n",
        "    padrao_item = re.compile(\n",
        "        r\"(\\d{3})\\s*\\n?\\s*\"               # Grupo 1: Número do item\n",
        "        r\"(AR\\d+)\\s+\"                     # Grupo 2: Código do produto\n",
        "        r\"([\\s\\S]+?)\\s+\"                  # Grupo 3: Descrição do produto (non-greedy)\n",
        "        r\"(\\d+(?:[.,]\\d+)?)\\s+\"            # Grupo 4: Quantidade\n",
        "        r\"(?:BDJ|CXT|CX|CXA|DZ|EMB|FRC|GFA|G|GR|KG|KIT|L|LT|ML|PAR|PC|PCT|PTE|RL|SC|TBO|UN|UND|VDO)\\d*\\s+X\\s+\" # Unidade (não capturada)\n",
        "        r\"([\\s\\S]+?)\"                     # Grupo 5: O resto do texto do item (preços)\n",
        "        r\"(?=\\s*\\n?\\s*\\d{3}\\s*\\n?\\s*AR|\\nTotal bruto de Itens)\", # Lookahead para o próximo item\n",
        "        re.DOTALL\n",
        "    )\n",
        "\n",
        "    dados_cupom = {\"estabelecimento\": {}, \"cupom\": {}, \"itens\": []}\n",
        "\n",
        "    # Extrai o nome do estabelecimento\n",
        "    linhas_brutas = texto_bruto.split('\\n')\n",
        "    if len(linhas_brutas) > 1:\n",
        "        dados_cupom[\"estabelecimento\"][\"nome\"] = linhas_brutas[1].strip()\n",
        "        dados_cupom[\"estabelecimento\"][\"nome_fantasia\"] = linhas_brutas[0].strip()\n",
        "\n",
        "    # Extrai a data e hora\n",
        "    match_data_hora = padrao_data_hora.search(texto_bruto)\n",
        "    if match_data_hora:\n",
        "        dados_cupom[\"cupom\"][\"data_hora\"] = match_data_hora.group(1).strip()\n",
        "\n",
        "    # Encontra todos os itens usando a regex principal\n",
        "    items_encontrados = padrao_item.findall(texto_bruto)\n",
        "\n",
        "    for item in items_encontrados:\n",
        "        try:\n",
        "            string_precos = item[4]\n",
        "            numeros_encontrados = re.findall(r'[\\d.,]+', string_precos)\n",
        "\n",
        "            valor_total = 0.0\n",
        "            valor_unitario = 0.0\n",
        "\n",
        "            if len(numeros_encontrados) >= 2:\n",
        "                valor_total = normalizar_valor(numeros_encontrados[0])\n",
        "                valor_unitario = normalizar_valor(numeros_encontrados[1])\n",
        "            elif len(numeros_encontrados) == 1:\n",
        "                valor_total = normalizar_valor(numeros_encontrados[0])\n",
        "                valor_unitario = valor_total\n",
        "\n",
        "            dados_cupom[\"itens\"].append({\n",
        "                \"codigo\": item[1].strip(),\n",
        "                \"descricao\": re.sub(r'\\s+', ' ', item[2].replace('\\n', ' ')).strip(),\n",
        "                \"quantidade\": normalizar_valor(item[3]),\n",
        "                \"valor_unitario\": valor_unitario,\n",
        "                \"valor_total_item\": valor_total\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao processar item do Atacadão: {item} -> {e}\")\n",
        "            continue\n",
        "\n",
        "    return dados_cupom\n",
        "# ---------------------------------------------------- PÃO DE AÇUCAR ------------------------------------------------------------\n",
        "def normalizar_valor(valor_str: str) -> float:\n",
        "    \"\"\"Função auxiliar para converter uma string de valor monetário para float.\"\"\"\n",
        "    if isinstance(valor_str, (int, float)):\n",
        "        return float(valor_str)\n",
        "    return float(valor_str.strip().replace('.', '').replace(',', '.'))\n",
        "\n",
        "def analisar_cupom_pao_de_acucar(texto_bruto: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Analisa o texto bruto de um cupom fiscal do Grupo Pão de Açúcar,\n",
        "    lidando com formatações de OCR inconsistentes.\n",
        "    \"\"\"\n",
        "    padrao_data_hora = re.compile(r\"(\\d{2}/\\d{2}/\\d{4}\\s+\\d{2}:\\d{2}:\\d{2})\")\n",
        "    padrao_item = re.compile(\n",
        "        r\"(\\d{3})\\s+(\\d+)\\s+(.+?)\\s+(\\d+(?:[.,]\\d+)?)\\s+([a-zA-Z]+)\\s+X\\s+([\\s\\S]+?)(?=\\n?\\s*\\d{3}\\s+\\d+|Total bruto de Itens)\",\n",
        "        re.DOTALL\n",
        "    )\n",
        "\n",
        "    dados_cupom = {\"estabelecimento\": {}, \"cupom\": {}, \"itens\": []}\n",
        "\n",
        "    match_nome = re.search(r\"COMPANHIA BRASILEIRA DE DISTRIBUICAO\", texto_bruto)\n",
        "    if match_nome:\n",
        "        dados_cupom[\"estabelecimento\"][\"nome\"] = match_nome.group(0).strip()\n",
        "\n",
        "    match_cnpj = re.search(r\"CNPJ\\s+([\\d./-]+)\", texto_bruto)\n",
        "    if match_cnpj:\n",
        "        dados_cupom[\"estabelecimento\"][\"cnpj\"] = match_cnpj.group(1).strip()\n",
        "\n",
        "    match_data_hora = re.search(padrao_data_hora, texto_bruto)\n",
        "    if match_data_hora:\n",
        "        dados_cupom[\"cupom\"][\"data_hora\"] = match_data_hora.group(1).strip()\n",
        "\n",
        "    items_encontrados = padrao_item.findall(texto_bruto)\n",
        "\n",
        "    for match in items_encontrados:\n",
        "        try:\n",
        "            string_precos = match[5]\n",
        "            numeros = re.findall(r'[\\d.,]+', string_precos)\n",
        "\n",
        "            valor_unitario = normalizar_valor(numeros[0]) if numeros else 0.0\n",
        "            valor_total = normalizar_valor(numeros[-1]) if numeros else 0.0\n",
        "\n",
        "            dados_cupom[\"itens\"].append({\n",
        "                \"codigo\": match[1],\n",
        "                \"descricao\": match[2].strip().replace('\\n', ' '),\n",
        "                \"quantidade\": normalizar_valor(match[3]),\n",
        "                \"valor_unitario\": valor_unitario,\n",
        "                \"valor_total_item\": valor_total\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao processar item do Pão de Açúcar: {match} -> {e}\")\n",
        "            continue\n",
        "\n",
        "    return dados_cupom\n",
        "\n",
        "# ---------------------------------------------------- MERCADO SÃO LUIZ  --------------------------------------------------------\n",
        "def normalizar_valor(valor_str: str) -> float:\n",
        "    \"\"\"Converte '1.234,56' -> 1234.56 e similares.\"\"\"\n",
        "    if isinstance(valor_str, (int, float)):\n",
        "        return float(valor_str)\n",
        "    s = str(valor_str).strip()\n",
        "    if ',' in s:\n",
        "        s = s.replace('.', '').replace(',', '.')\n",
        "    return float(s)\n",
        "\n",
        "def analisar_cupom_sao_luiz(texto_bruto: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Parser para o cupom do São Luiz com suporte a itens multi-linha.\n",
        "    Captura corretamente o VL ITEM R$ (total do item), que muitas vezes vem na linha seguinte.\n",
        "    \"\"\"\n",
        "    dados_cupom = {\n",
        "        \"supermercado\": \"São Luiz\",\n",
        "        \"estabelecimento\": {},\n",
        "        \"cupom\": {},\n",
        "        \"itens\": []\n",
        "    }\n",
        "\n",
        "    # ——— Cabeçalho básico (opcional) ———\n",
        "    m_nome = re.search(r\"SUPER\\s+MERCADINHOS\\s+SAO\\s+LUIZ\", texto_bruto, flags=re.I)\n",
        "    if m_nome:\n",
        "        dados_cupom[\"estabelecimento\"][\"nome\"] = m_nome.group(0).strip()\n",
        "\n",
        "    m_cnpj = re.search(r\"CNPJ\\s+([\\d./-]+)\", texto_bruto)\n",
        "    if m_cnpj:\n",
        "        dados_cupom[\"estabelecimento\"][\"cnpj\"] = m_cnpj.group(1).strip()\n",
        "\n",
        "    m_data_hora = re.search(r\"(\\d{2}/\\d{2}/\\d{4}\\s+\\d{2}:\\d{2}:\\d{2})\", texto_bruto)\n",
        "    if m_data_hora:\n",
        "        dados_cupom[\"cupom\"][\"data_hora\"] = m_data_hora.group(1)\n",
        "\n",
        "    # ——— Isolar seção de itens ———\n",
        "    try:\n",
        "        # Corta tudo antes da linha de cabeçalho da tabela\n",
        "        corpo = re.split(r\"#\\s*\\|\\s*COD\\s*\\|\\s*DESC.*?\\n\", texto_bruto, flags=re.I|re.S)[1]\n",
        "        # Corta no rodapé/somatório\n",
        "        corpo = re.split(r\"Total\\s+bruto\\s+de\\s+Itens\", corpo, flags=re.I)[0]\n",
        "    except IndexError:\n",
        "        return dados_cupom\n",
        "\n",
        "    # ——— Agrupar itens (cada item começa com 3 dígitos + código longo) ———\n",
        "    item_blocks: List[str] = []\n",
        "    current: List[str] = []\n",
        "    start_item_pattern = re.compile(r\"^\\s*\\d{3}\\s+\\d{5,}\", flags=re.M)\n",
        "\n",
        "    for linha in corpo.splitlines():\n",
        "        ll = linha.strip()\n",
        "        if not ll:\n",
        "            continue\n",
        "        if start_item_pattern.match(ll):\n",
        "            if current:\n",
        "                item_blocks.append(\" \".join(current))\n",
        "            current = [ll]\n",
        "        else:\n",
        "            if current:  # só agrega depois que um item começou\n",
        "                current.append(ll)\n",
        "    if current:\n",
        "        item_blocks.append(\" \".join(current))\n",
        "\n",
        "    # ——— Regex de extração robusta ———\n",
        "    # Ex.: \"002 0000000002791 PEITO PERU SADIA FAT 0.204 KG X 76,99 ( 5,25) 15,71\"\n",
        "    padrao = re.compile(\n",
        "        r\"^\\s*\\d{3}\\s+\"                              # nº sequencial do item (ignorado)\n",
        "        r\"(?P<codigo>\\d{5,})\\s+\"                     # código\n",
        "        r\"(?P<descricao>.+?)\\s+\"                     # descrição (preguiçosa)\n",
        "        r\"(?P<quantidade>\\d+(?:[.,]\\d+)?)\\s+\"        # quantidade\n",
        "        r\"(?P<unidade>UN|KG|G)\\s+X\\s+\"               # unidade + 'X'\n",
        "        r\"(?P<valor_unitario>\\d+(?:[.,]\\d+)?)\"       # valor unitário\n",
        "        r\"(?:\\s*\\(\\s*(?P<valor_tributo>[\\d.,]+)\\s*\\))?\" # tributo opcional (entre parênteses)\n",
        "        r\"(?:\\s+(?P<valor_item>[\\d.,]+))?\",          # total do item (VL ITEM R$), geralmente após parênteses\n",
        "        flags=re.I | re.S\n",
        "    )\n",
        "\n",
        "    for bloco in item_blocks:\n",
        "        m = padrao.search(bloco)\n",
        "        if not m:\n",
        "            # Fallback: tentar pegar último número como total e primeiro após ' X ' como unitário\n",
        "            # (Evita perder item em algum layout estranho)\n",
        "            numeros = re.findall(r\"[\\d]+(?:[.,]\\d+)?\", bloco)\n",
        "            if not numeros:\n",
        "                continue\n",
        "            # tentar extrair campos essenciais\n",
        "            cab = re.match(r\"^\\s*(\\d{3})\\s+(\\d{5,})\\s+(.*)$\", bloco)\n",
        "            if not cab:\n",
        "                continue\n",
        "            codigo = cab.group(2)\n",
        "            # heurística para separar descrição até antes da quantidade/unidade\n",
        "            desc_q = cab.group(3)\n",
        "            q_match = re.search(r\"\\s(\\d+(?:[.,]\\d+)?)\\s+(UN|KG|G)\\b\", desc_q)\n",
        "            if not q_match:\n",
        "                continue\n",
        "            descricao = desc_q[:q_match.start()].strip()\n",
        "            quantidade = normalizar_valor(q_match.group(1))\n",
        "            unidade = q_match.group(2).upper()\n",
        "            # preços (heurística)\n",
        "            valor_unitario = normalizar_valor(numeros[0])\n",
        "            valor_item = normalizar_valor(numeros[-1])\n",
        "        else:\n",
        "            g = m.groupdict()\n",
        "            codigo = g[\"codigo\"]\n",
        "            descricao = g[\"descricao\"].strip()\n",
        "            quantidade = normalizar_valor(g[\"quantidade\"])\n",
        "            unidade = g[\"unidade\"].upper()\n",
        "            valor_unitario = normalizar_valor(g[\"valor_unitario\"])\n",
        "            # se o VL ITEM não veio capturado, usa último número do bloco como fallback\n",
        "            if g.get(\"valor_item\"):\n",
        "                valor_item = normalizar_valor(g[\"valor_item\"])\n",
        "            else:\n",
        "                numeros = re.findall(r\"[\\d]+(?:[.,]\\d+)?\", bloco)\n",
        "                valor_item = normalizar_valor(numeros[-1]) if numeros else round(valor_unitario * quantidade, 2)\n",
        "\n",
        "        dados_cupom[\"itens\"].append({\n",
        "            \"codigo\": codigo,\n",
        "            \"descricao\": descricao,\n",
        "            \"quantidade\": quantidade,\n",
        "            \"unidade\": unidade,\n",
        "            \"valor_unitario\": valor_unitario,\n",
        "            \"valor_total_item\": valor_item\n",
        "        })\n",
        "\n",
        "    return dados_cupom\n",
        "\n",
        "# ---------------------------------------------------- SUPERMERCADO PINHEIRO ----------------------------------------------------\n",
        "def normalizar_valor(valor_str: str) -> float:\n",
        "    \"\"\"\n",
        "    Converte uma string de valor monetário para float,\n",
        "    lidando corretamente com separadores decimais e de milhares.\n",
        "    \"\"\"\n",
        "    if isinstance(valor_str, (int, float)):\n",
        "        return float(valor_str)\n",
        "\n",
        "    valor_limpo = valor_str.strip()\n",
        "\n",
        "    if ',' in valor_limpo:\n",
        "        valor_limpo = valor_limpo.replace('.', '')\n",
        "        valor_limpo = valor_limpo.replace(',', '.')\n",
        "\n",
        "    return float(valor_limpo)\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# NOVA FUNÇÃO DE EXTRAÇÃO PARA SUPERMERCADO PINHEIRO\n",
        "# ----------------------------------------------------------------\n",
        "def analisar_cupom_pinheiro(texto_bruto: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Analisa o texto de um cupom do Supermercado Pinheiro usando um método\n",
        "    de pré-processamento para agrupar linhas de itens fragmentados.\n",
        "    \"\"\"\n",
        "    dados_cupom = {\"supermercado\": \"Supermercado Pinheiro\", \"estabelecimento\": {}, \"cupom\": {}, \"itens\": []}\n",
        "\n",
        "    # 1. Extrai informações do cabeçalho\n",
        "    padrao_data_hora = re.compile(r\"(\\d{2}/\\d{2}/\\d{4}\\s+\\d{2}:\\d{2}:\\d{2})\")\n",
        "    match_nome = re.search(r\"SUPERMERCADO PINHEIRO\", texto_bruto)\n",
        "    if match_nome:\n",
        "        dados_cupom[\"estabelecimento\"][\"nome\"] = match_nome.group(0).strip()\n",
        "    match_cnpj = re.search(r\"CNPJ\\s+([\\d./-]+)\", texto_bruto)\n",
        "    if match_cnpj:\n",
        "        dados_cupom[\"estabelecimento\"][\"cnpj\"] = match_cnpj.group(1).strip()\n",
        "    match_data_hora = re.search(padrao_data_hora, texto_bruto)\n",
        "    if match_data_hora:\n",
        "        dados_cupom[\"cupom\"][\"data_hora\"] = match_data_hora.group(1).strip()\n",
        "\n",
        "    # 2. Pré-processamento: Agrupa as linhas de cada item\n",
        "    item_blocks = []\n",
        "    current_block = \"\"\n",
        "    start_item_pattern = re.compile(r\"^\\d{3}\\s+\\d+\") # Padrão para o início de um item\n",
        "\n",
        "    secao_itens_texto = \"\"\n",
        "    try:\n",
        "        secao_itens_texto = re.split(r'#\\s*\\|\\s*COD\\s*\\|\\s*DESC', texto_bruto, flags=re.IGNORECASE)[1]\n",
        "        secao_itens_texto = re.split(r'Total bruto de Itens', secao_itens_texto, flags=re.IGNORECASE)[0]\n",
        "    except IndexError:\n",
        "        return dados_cupom\n",
        "\n",
        "    for linha in secao_itens_texto.split('\\n'):\n",
        "        linha_limpa = linha.strip()\n",
        "        if not linha_limpa:\n",
        "            continue\n",
        "\n",
        "        if start_item_pattern.match(linha_limpa):\n",
        "            if current_block:\n",
        "                item_blocks.append(current_block)\n",
        "            current_block = linha_limpa\n",
        "        elif current_block:\n",
        "            current_block += \" \" + linha_limpa\n",
        "\n",
        "    if current_block:\n",
        "        item_blocks.append(current_block)\n",
        "\n",
        "    # 3. Extração: Aplica uma regex a cada bloco de item já reconstruído\n",
        "    padrao_extracao = re.compile(\n",
        "        r\"(\\d{3})\\s+\"           # Grupo 1: Número do item\n",
        "        r\"(\\d+)\\s+\"              # Grupo 2: Código do produto\n",
        "        r\"(.+?)\\s+\"              # Grupo 3: Descrição\n",
        "        r\"(\\d+(?:[.,]\\d+)?)\\s+\"  # Grupo 4: Quantidade\n",
        "        r\"([a-zA-Z]+)\\s+X\\s+\"     # Grupo 5: Unidade\n",
        "        r\"([\\s\\S]+)\"             # Grupo 6: Resto (string de preços)\n",
        "    )\n",
        "\n",
        "    for bloco in item_blocks:\n",
        "        match = padrao_extracao.match(bloco)\n",
        "        if match:\n",
        "            try:\n",
        "                string_precos = match.group(6)\n",
        "                numeros = re.findall(r'[\\d.,]+', string_precos)\n",
        "\n",
        "                valor_unitario = normalizar_valor(numeros[0]) if numeros else 0.0\n",
        "                valor_total = normalizar_valor(numeros[-1]) if numeros else 0.0\n",
        "\n",
        "                dados_cupom[\"itens\"].append({\n",
        "                    \"codigo\": match.group(2),\n",
        "                    \"descricao\": match.group(3).strip(),\n",
        "                    \"quantidade\": normalizar_valor(match.group(4)),\n",
        "                    \"valor_unitario\": valor_unitario,\n",
        "                    \"valor_total_item\": valor_total\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao processar bloco do Pinheiro: {bloco} -> {e}\")\n",
        "                continue\n",
        "\n",
        "    return dados_cupom\n",
        "\n",
        "# ---------------------------------------------------- SUPERMERCADO GUARA -------------------------------------------------------\n",
        "def normalizar_valor(valor_str: str) -> float:\n",
        "    \"\"\"\n",
        "    Converte uma string de valor monetário para float,\n",
        "    lidando corretamente com separadores decimais e de milhares.\n",
        "    \"\"\"\n",
        "    if isinstance(valor_str, (int, float)):\n",
        "        return float(valor_str)\n",
        "\n",
        "    valor_limpo = valor_str.strip()\n",
        "\n",
        "    if ',' in valor_limpo:\n",
        "        valor_limpo = valor_limpo.replace('.', '')\n",
        "        valor_limpo = valor_limpo.replace(',', '.')\n",
        "\n",
        "    return float(valor_limpo)\n",
        "\n",
        "def analisar_cupom_guara(texto_bruto: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Analisa o texto bruto de um cupom fiscal do Supermercado Guara.\n",
        "    \"\"\"\n",
        "    padrao_data_hora = re.compile(r\"(\\d{2}/\\d{2}/\\d{4}\\s+\\d{2}:\\d{2}:\\d{2})\")\n",
        "\n",
        "    # Regex para o formato de item do Guara.\n",
        "    # Estrutura: NUM | COD | DESC | QTD | UN | VL UN | (TAX) | VL ITEM\n",
        "    padrao_item = re.compile(\n",
        "        r\"^(\\d{3})\\s+\"                      # Grupo 1: Número do item\n",
        "        r\"(\\d+)\\s+\"                         # Grupo 2: Código do produto\n",
        "        r\"(.+?)\\s+\"                         # Grupo 3: Descrição\n",
        "        r\"(\\d+(?:[.,]\\d+)?)\\s+\"             # Grupo 4: Quantidade\n",
        "        r\"([a-zA-Z]+)\\s+X\\s+\"                # Grupo 5: Unidade\n",
        "        r\"([\\d.,]+)\\s+\"                     # Grupo 6: Valor unitário\n",
        "        r\"\\([\\s\\d.,]+\\)\\s+\"                 # Taxa entre parênteses (não capturada)\n",
        "        r\"([\\d.,]+)$\"                       # Grupo 7: Valor total do item\n",
        "    )\n",
        "\n",
        "    dados_cupom = {\"supermercado\": \"Supermercado Guara\", \"estabelecimento\": {}, \"cupom\": {}, \"itens\": []}\n",
        "\n",
        "    match_nome = re.search(r\"SUPERMERCADO GUARA LTDA\", texto_bruto)\n",
        "    if match_nome:\n",
        "        dados_cupom[\"estabelecimento\"][\"nome\"] = match_nome.group(0).strip()\n",
        "\n",
        "    match_cnpj = re.search(r\"CNPJ\\s+([\\d./-]+)\", texto_bruto)\n",
        "    if match_cnpj:\n",
        "        dados_cupom[\"estabelecimento\"][\"cnpj\"] = match_cnpj.group(1).strip()\n",
        "\n",
        "    match_data_hora = re.search(padrao_data_hora, texto_bruto)\n",
        "    if match_data_hora:\n",
        "        dados_cupom[\"cupom\"][\"data_hora\"] = match_data_hora.group(1).strip()\n",
        "\n",
        "    # Processa cada linha do texto para encontrar os itens\n",
        "    for linha in texto_bruto.split('\\n'):\n",
        "        match = padrao_item.match(linha.strip())\n",
        "        if match:\n",
        "            try:\n",
        "                dados_cupom[\"itens\"].append({\n",
        "                    \"codigo\": match.group(2),\n",
        "                    \"descricao\": match.group(3).strip(),\n",
        "                    \"quantidade\": normalizar_valor(match.group(4)),\n",
        "                    \"valor_unitario\": normalizar_valor(match.group(6)),\n",
        "                    \"valor_total_item\": normalizar_valor(match.group(7))\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao processar item do Guara: {linha} -> {e}\")\n",
        "                continue\n",
        "\n",
        "    return dados_cupom\n",
        "\n",
        "# ---------------------------------------------------- SUPERMERCADO COMETA -------------------------------------------------------\n",
        "def normalizar_valor(valor_str: str) -> float:\n",
        "    \"\"\"\n",
        "    Converte uma string de valor monetário para float.\n",
        "    \"\"\"\n",
        "    if isinstance(valor_str, (int, float)):\n",
        "        return float(valor_str)\n",
        "\n",
        "    valor_limpo = valor_str.strip()\n",
        "    if ',' in valor_limpo:\n",
        "        valor_limpo = valor_limpo.replace('.', '')\n",
        "        valor_limpo = valor_limpo.replace(',', '.')\n",
        "\n",
        "    return float(valor_limpo)\n",
        "\n",
        "def analisar_cupom_cometa(texto_bruto: str) -> Dict[str, Any]:\n",
        "    dados_cupom = {\"supermercado\": \"Cometa Supermercado\", \"estabelecimento\": {}, \"cupom\": {}, \"itens\": []}\n",
        "\n",
        "    # 1. Extrai informações do cabeçalho\n",
        "    m_nome = re.search(r\"SUPERMERCADO COMETA LTDA\", texto_bruto)\n",
        "    if m_nome:\n",
        "        dados_cupom[\"estabelecimento\"][\"nome\"] = m_nome.group(0).strip()\n",
        "    m_cnpj = re.search(r\"CNPJ\\s+([\\d./-]+)\", texto_bruto)\n",
        "    if m_cnpj:\n",
        "        dados_cupom[\"estabelecimento\"][\"cnpj\"] = m_cnpj.group(1).strip()\n",
        "    m_data_hora = re.search(r\"(\\d{2}/\\d{2}/\\d{4}\\s+\\d{2}:\\d{2}:\\d{2})\", texto_bruto)\n",
        "    if m_data_hora:\n",
        "        dados_cupom[\"cupom\"][\"data_hora\"] = m_data_hora.group(1)\n",
        "\n",
        "    # 2. Isola a seção de itens\n",
        "    try:\n",
        "        corpo = re.split(r\"#\\s*\\|\\s*COD\\s*\\|\\s*DESC.*?\\n\", texto_bruto, flags=re.I|re.S)[1]\n",
        "        corpo = re.split(r\"Total\\s+bruto\\s+de\\s+Itens\", corpo, flags=re.I)[0]\n",
        "    except IndexError:\n",
        "        return dados_cupom\n",
        "\n",
        "    # 3. Agrupa itens multi-linha\n",
        "    item_blocks: List[str] = []\n",
        "    current: List[str] = []\n",
        "    start_item_pattern = re.compile(r\"^\\s*\\d{3}\\s+\\d{2,}\", flags=re.M)\n",
        "\n",
        "    for linha in corpo.splitlines():\n",
        "        ll = linha.strip()\n",
        "        if not ll:\n",
        "            continue\n",
        "        if start_item_pattern.match(ll):\n",
        "            if current:\n",
        "                item_blocks.append(\" \".join(current))\n",
        "            current = [ll]\n",
        "        else:\n",
        "            if current:\n",
        "                current.append(ll)\n",
        "    if current:\n",
        "        item_blocks.append(\" \".join(current))\n",
        "\n",
        "    # 4. Regex para extrair campos\n",
        "    padrao = re.compile(\n",
        "        r\"^\\s*\\d{3}\\s+\"                               # Nº do item\n",
        "        r\"(?P<codigo>\\d+)\\s+\"                         # Código\n",
        "        r\"(?P<descricao>.+?)\\s+\"                      # Descrição\n",
        "        r\"(?P<quantidade>\\d+(?:[.,]\\d+)?)\\s+\"         # Quantidade\n",
        "        r\"(?P<unidade>UN|KG|G)\\s+X\\s+\"                # Unidade\n",
        "        r\"(?P<valor_unitario>\\d+(?:[.,]\\d+)?)\"        # Valor unitário\n",
        "        r\"(?:\\s*\\(\\s*[\\d.,]+\\))?\"                     # Tributo opcional\n",
        "        r\"(?:\\s+(?P<valor_item>[\\d.,]+))?\",           # Total do item\n",
        "        flags=re.I | re.S\n",
        "    )\n",
        "\n",
        "    for bloco in item_blocks:\n",
        "        m = padrao.search(bloco)\n",
        "        if not m:\n",
        "            # fallback\n",
        "            numeros = re.findall(r\"[\\d]+(?:[.,]\\d+)?\", bloco)\n",
        "            if not numeros:\n",
        "                continue\n",
        "            cab = re.match(r\"^\\s*\\d{3}\\s+(\\d+)\\s+(.*)$\", bloco)\n",
        "            if not cab:\n",
        "                continue\n",
        "            codigo = cab.group(1)\n",
        "            desc_q = cab.group(2)\n",
        "            q_match = re.search(r\"\\s(\\d+(?:[.,]\\d+)?)\\s+(UN|KG|G)\\b\", desc_q)\n",
        "            if not q_match:\n",
        "                continue\n",
        "            descricao = desc_q[:q_match.start()].strip()\n",
        "            quantidade = normalizar_valor(q_match.group(1))\n",
        "            unidade = q_match.group(2).upper()\n",
        "            valor_unitario = normalizar_valor(numeros[0])\n",
        "            valor_item = normalizar_valor(numeros[-1])\n",
        "        else:\n",
        "            g = m.groupdict()\n",
        "            codigo = g[\"codigo\"]\n",
        "            descricao = g[\"descricao\"].strip()\n",
        "            quantidade = normalizar_valor(g[\"quantidade\"])\n",
        "            unidade = g[\"unidade\"].upper()\n",
        "            valor_unitario = normalizar_valor(g[\"valor_unitario\"])\n",
        "            if g.get(\"valor_item\"):\n",
        "                valor_item = normalizar_valor(g[\"valor_item\"])\n",
        "            else:\n",
        "                numeros = re.findall(r\"[\\d]+(?:[.,]\\d+)?\", bloco)\n",
        "                valor_item = normalizar_valor(numeros[-1]) if numeros else round(valor_unitario * quantidade, 2)\n",
        "\n",
        "        dados_cupom[\"itens\"].append({\n",
        "            \"codigo\": codigo,\n",
        "            \"descricao\": descricao,\n",
        "            \"quantidade\": quantidade,\n",
        "            \"unidade\": unidade,\n",
        "            \"valor_unitario\": valor_unitario,\n",
        "            \"valor_total_item\": valor_item\n",
        "        })\n",
        "\n",
        "    return dados_cupom\n",
        "\n",
        "# ---------------------------------------------------- SAM'S CLUB ---------------------------------------------------------------\n",
        "# via INSERT SQL\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. FUNÇÃO PRINCIPAL DE ANÁLISE (O \"ROTEADOR\")\n",
        "# ==============================================================================\n",
        "def analisar_cupom(texto_bruto: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Identifica o supermercado a partir do texto e chama a função de extração correta.\n",
        "    \"\"\"\n",
        "    texto_upper = texto_bruto.upper()\n",
        "\n",
        "    if \"COMETA SUPERMERCADO\" in texto_upper:\n",
        "        print(\"Identificado: COMETA. Usando parser específico.\")\n",
        "        return analisar_cupom_cometa(texto_bruto)\n",
        "\n",
        "    elif \"SUPERMERCADO GUARA\" in texto_upper:\n",
        "        print(\"Identificado: GUARA. Usando parser específico.\")\n",
        "        return analisar_cupom_guara(texto_bruto)\n",
        "\n",
        "    elif \"SUPERMERCADO PINHEIRO\" in texto_upper:\n",
        "        print(\"Identificado: PINHEIRO. Usando parser específico.\")\n",
        "        return analisar_cupom_pinheiro(texto_bruto)\n",
        "\n",
        "    elif \"SUPER MERCADINHOS SAO LUIZ\" in texto_upper:\n",
        "        print(\"Identificado: SÃO LUIZ. Usando parser específico.\")\n",
        "        return analisar_cupom_sao_luiz(texto_bruto)\n",
        "        return {\"supermercado\": \"São Luiz\", \"itens\": []} # Placeholder\n",
        "\n",
        "    elif \"ASSAI\" in texto_upper:\n",
        "        print(\"Identificado: ASSAI. Usando parser específico.\")\n",
        "        return analisar_cupom_assai(texto_bruto)\n",
        "        return {\"supermercado\": \"Assaí\", \"itens\": []} # Placeholder\n",
        "\n",
        "    elif \"MAXXI\" in texto_upper or \"WMS SUPERMERCADOS DO BRASIL\" in texto_upper:\n",
        "        print(\"Identificado: ATACADAO/MAXXI. Usando parser específico.\")\n",
        "        return analisar_cupom_atacadao(texto_bruto)\n",
        "        return {\"supermercado\": \"Atacadão/Maxxi\", \"itens\": []} # Placeholder\n",
        "\n",
        "    elif \"COMPANHIA BRASILEIRA DE DISTRIBUICAO\" in texto_upper:\n",
        "        print(\"Identificado: PÃO DE AÇUCAR. Usando parser específico.\")\n",
        "        return analisar_cupom_pao_de_acucar(texto_bruto)\n",
        "        return {\"supermercado\": \"Pão de Açúcar\", \"itens\": []} # Placeholder\n",
        "\n",
        "    else:\n",
        "        print(\"AVISO: Supermercado não reconhecido. Nenhum item será extraído.\")\n",
        "        return { \"supermercado\": \"Não Identificado\", \"estabelecimento\": {}, \"cupom\": {}, \"itens\": [] }\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. FUNÇÃO DE TRANSFORMAÇÃO PARA DATAFRAME (COM TRATAMENTO AVANÇADO)\n",
        "# ==============================================================================\n",
        "def transformar_dados_para_df(dados_cupom, nome_arquivo_pdf):\n",
        "    # Dicionário para normalizar os nomes dos estabelecimentos\n",
        "    normalizacao_estabelecimentos = {\n",
        "        \"WMS SUPERMERCADOS DO BRASIL LTDA\": \"Atacadão\",\n",
        "        \"ASSAI\": \"Assaí\",\n",
        "        \"COMPANHIA BRASILEIRA DE DISTRIBUICAO\": \"Pão de Açúcar\",\n",
        "        \"SUPERMERCADO PINHEIRO\": \"Supermercado Pinheiro\",\n",
        "        \"SUPERMERCADO GUARA LTDA\": \"Supermercado Guara\",\n",
        "        \"SUPERMERCADO COMETA LTDA\": \"Supermercado Cometa\",\n",
        "        \"SUPER MERCADINHOS SAO LUIZ\": \"São Luiz\"\n",
        "    }\n",
        "\n",
        "    # Dicionário completo de substituições para descrições (resumido aqui)\n",
        "    substituicoes = biblioteca_produtos\n",
        "\n",
        "    # Função para aplicar substituições tolerantes\n",
        "    def tratar_descricao(descricao):\n",
        "        desc = re.sub(r'\\s+', ' ', descricao).strip()\n",
        "        for chave, valor in substituicoes.items():\n",
        "            # Cria padrão tolerante (pontos e espaços)\n",
        "            padrao = re.escape(chave)\n",
        "            padrao = padrao.replace(r'\\.', r'[\\s\\.]*').replace(' ', r'[\\s]+')\n",
        "            regex = re.compile(r'\\b' + padrao + r'\\b', flags=re.I)\n",
        "            desc = regex.sub(valor, desc)\n",
        "        # Remove espaços extras\n",
        "        return re.sub(r'\\s+', ' ', desc).strip()\n",
        "\n",
        "    linhas_df = []\n",
        "\n",
        "    # Normalização do nome do estabelecimento\n",
        "    local_compra = dados_cupom.get(\"estabelecimento\", {}).get(\"nome\", \"Não informado\").upper()\n",
        "    for chave, valor in normalizacao_estabelecimentos.items():\n",
        "        if chave in local_compra:\n",
        "            local_compra = valor\n",
        "            break\n",
        "\n",
        "    # Data da compra\n",
        "    data_hora_str = dados_cupom.get(\"cupom\", {}).get(\"data_hora\")\n",
        "    try:\n",
        "        dt_compra_formatada = datetime.strptime(data_hora_str.strip(), '%d/%m/%Y %H:%M:%S').strftime('%Y-%m-%d') \\\n",
        "            if data_hora_str else datetime.now().strftime('%Y-%m-%d')\n",
        "    except (ValueError, TypeError):\n",
        "        dt_compra_formatada = datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "    # Monta as linhas para o DataFrame\n",
        "    for item in dados_cupom.get(\"itens\", []):\n",
        "        descricao_original = item.get(\"descricao\", \"\")\n",
        "        descricao_tratada = tratar_descricao(descricao_original)\n",
        "\n",
        "        linha = {\n",
        "            \"arquivo\": nome_arquivo_pdf,\n",
        "            \"local_da_compra\": local_compra,\n",
        "            \"dt_compra\": dt_compra_formatada,\n",
        "            \"codigo\": item.get(\"codigo\", \"N/A\"),\n",
        "            \"descricao\": descricao_original,\n",
        "            \"descricao_tratada\": descricao_tratada,\n",
        "            \"quantidade\": item.get(\"quantidade\"),\n",
        "            \"preco_unitario\": item.get(\"valor_unitario\"),\n",
        "            \"preco_total\": item.get(\"valor_total_item\"),\n",
        "        }\n",
        "        linhas_df.append(linha)\n",
        "\n",
        "    return linhas_df\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. BLOCO PRINCIPAL DE EXECUÇÃO\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    pasta_de_trabalho = \"/content/\"\n",
        "    todos_os_itens_df_list = []\n",
        "\n",
        "    print(f\"Iniciando varredura de arquivos .pdf em '{pasta_de_trabalho}'...\")\n",
        "    arquivos_pdf = [f for f in os.listdir(pasta_de_trabalho) if f.lower().endswith('.pdf')]\n",
        "\n",
        "    if not arquivos_pdf:\n",
        "        print(\"Nenhum arquivo .pdf encontrado. Encerrando o script.\")\n",
        "    else:\n",
        "        # Etapa 1: Loop para ler e processar cada arquivo PDF individualmente\n",
        "        for nome_arquivo in sorted(arquivos_pdf):\n",
        "            caminho_completo = os.path.join(pasta_de_trabalho, nome_arquivo)\n",
        "            print(f\"\\nProcessando arquivo: {nome_arquivo}...\")\n",
        "\n",
        "            texto_bruto = extrair_texto_do_pdf(caminho_completo)\n",
        "            if texto_bruto:\n",
        "                dados_analisados = analisar_cupom(texto_bruto)\n",
        "\n",
        "                dados_para_df = transformar_dados_para_df(dados_analisados, nome_arquivo)\n",
        "                if dados_para_df:\n",
        "                    df_temp = pd.DataFrame(dados_para_df)\n",
        "                    todos_os_itens_df_list.append(df_temp)\n",
        "\n",
        "                print(f\"Encontrados {len(dados_para_df)} itens no arquivo.\")\n",
        "\n",
        "        # Etapa 2: Consolidação dos dados após processar todos os arquivos\n",
        "        if not todos_os_itens_df_list:\n",
        "            print(\"\\nNenhum item foi extraído dos arquivos PDF. Encerrando.\")\n",
        "        else:\n",
        "            # Junta todos os dataframes de cada arquivo em um único dataframe mestre\n",
        "            df = pd.concat(todos_os_itens_df_list, ignore_index=True)\n",
        "            df[\"data_insercao\"] = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "            print(\"\\nTodos os arquivos foram processados. Preparando para enviar ao Supabase.\")\n",
        "\n",
        "            # Etapa 3: Sincronização com o Supabase (com o log detalhado)\n",
        "            try:\n",
        "                arquivos_unicos = df[\"arquivo\"].unique()\n",
        "                print(f\"\\nEncontrados {len(arquivos_unicos)} arquivos únicos para sincronizar.\")\n",
        "\n",
        "                for nome_arquivo in sorted(arquivos_unicos):\n",
        "                    print(\"─\" * 50)\n",
        "                    print(f\"Sincronizando dados do arquivo: {nome_arquivo}\")\n",
        "\n",
        "                    df_arquivo_atual = df[df[\"arquivo\"] == nome_arquivo]\n",
        "                    dados_para_inserir = df_arquivo_atual.to_dict(orient=\"records\")\n",
        "\n",
        "                    print(f\"Apagando registros antigos de '{nome_arquivo}'...\")\n",
        "                    delete_response = supabase.table(\"supermercados_json\").delete().eq(\"arquivo\", nome_arquivo).execute()\n",
        "                    print(\"Registros antigos apagados (ou não existiam).\")\n",
        "\n",
        "                    print(f\"Inserindo {len(dados_para_inserir)} novos registros...\")\n",
        "                    insert_response = supabase.table(\"supermercados_json\").insert(dados_para_inserir).execute()\n",
        "                    print(\"Novos registros inseridos com sucesso!\")\n",
        "\n",
        "                print(\"\\n\" + \"=\"*50)\n",
        "                print(\"SINCRONIZAÇÃO COM SUPABASE CONCLUÍDA!\")\n",
        "                print(f\"Horário de execução: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n",
        "                print(\"=\"*50)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nERRO CRÍTICO durante a sincronização com o Supabase: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCaYHRv8Dzu_",
        "outputId": "b8635c50-2674-4d36-e40f-0fbd2021615f",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conexão com Supabase estabelecida com sucesso.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qk9WTiPaBNu5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
